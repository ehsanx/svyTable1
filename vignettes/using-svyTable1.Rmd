---
title: "Using svyTable1 for Survey-Weighted Summary Tables"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using svyTable1 for Survey-Weighted Summary Tables}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: yes
---

```{r setup}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The svyTable1 package provides a straightforward function, `svytable1()`, to generate descriptive statistics tables ("Table 1") from complex survey data. This vignette demonstrates its use, explains its default "mixed mode" display, and details its advanced reliability checking features.

## Understanding the Mixed Mode (A Best Practice)

The `svytable1` function defaults to `mode = "mixed"`. This approach is considered a best practice in high-quality survey research for several key reasons rooted in transparency.

When you present a table in mixed mode, you provide two types of information for each categorical variable:

- **The Unweighted N:** This is the actual number of people you interviewed in that category. It tells the reader about the statistical precision of the estimate [@seidenberg2023preferred]. A percentage based on 10 people is much less reliable than one based on 1,000 people.
- **The Weighted Percentage (%):** This is an estimate of the proportion of the entire target population that falls into that category, adjusted for the complex survey design (e.g., oversampling of certain groups) [@national1994plan].

By showing both, you allow your audience to understand the crucial distinction between your sample and the population you are making inferences about. It also reveals the impact of the survey weights—if a group has a small N but a large weighted percentage, it signals that the estimate for that group relies heavily on up-weighting a few individuals, which may affect the stability of the estimate.

## Example: Using NHANES Data (2009–2012)

### 1. Data Preparation

```{r}
library(svyTable1)
library(survey)
library(dplyr)
library(NHANES)

# Load the raw NHANES data (2009-2012)
data(NHANESraw)

# Prepare data for adults, keeping NAs
nhanes_adults_with_na <- NHANESraw %>%
  dplyr::filter(Age >= 20) %>%
  mutate(
    ObeseStatus = factor(ifelse(BMI >= 30, "Obese", "Not Obese"),
                         levels = c("Not Obese", "Obese"))
  )

# Create the survey design object on the data that still contains NAs
adult_design_with_na <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = nhanes_adults_with_na
)
```

### 2. Generating Tables

#### Example A: Handling Variables with Missing Data

```{r}
# Define a set of variables, some with expected missing values
vars_with_missing <- c(
  "Age", 
  "Gender", 
  "Race1", 
  "Education",
  "HHIncome", 
  "TotChol",       
  "SleepHrsNight", 
  "SmokeNow"       
)

# Generate the table using the full design object (with NAs)
table_with_missing <- svytable1(
  design = adult_design_with_na, 
  strata_var = "ObeseStatus", 
  table_vars = vars_with_missing
)

# Display the table
knitr::kable(
  table_with_missing, 
  caption = "Table 1: Summarizing Variables with Missing Data"
)
```

It's important to note that `svytable1()` automatically detects missing (`NA`) values in the stratification variable. It treats these observations as a distinct group, creating a separate 'Missing' column in the table to ensure all data is accounted for.

#### Example B: Summarizing Complete Data

```{r}
library(tidyr)

# Define a set of core variables for a complete-case analysis
vars_for_complete_table <- c(
  "Age", 
  "Gender", 
  "Race1",
  "BPSysAve",
  "Pulse",
  "BMI"
)

# Create a new, complete-case data frame
nhanes_adults_complete <- nhanes_adults_with_na %>%
  drop_na(all_of(vars_for_complete_table))

# Create a new design object based on this complete-case data
adult_design_complete <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = nhanes_adults_complete
)

# Generate the table
table_without_missing <- svytable1(
  design = adult_design_complete, 
  strata_var = "ObeseStatus", 
  table_vars = c("Age", "Gender", "Race1", "BPSysAve", "Pulse")
)

# Display the table
knitr::kable(
  table_without_missing, 
  caption = "Table 2: Summarizing Variables with No Missing Data"
)
```

This table is "clean": it has no "Missing" rows or columns because the input data was complete for the selected variables.

## 3. Checking Estimate Reliability for Proportions (NCHS Standards)

When you publish research, it's crucial to ensure your results are statistically trustworthy. An estimate based on a very small number of people, for example, can be misleading. To help with this, `svytable1` can automatically apply the **NCHS Data Presentation Standards for Proportions**, which are a set of rules designed to prevent the publication of unreliable estimates.

You can activate this feature using two arguments:

- **`reliability_checks = TRUE`**: This tells the function to perform the checks. Any estimate that fails is replaced with an asterisk (*) in the final table.
- **`return_metrics = TRUE`**: This is highly recommended. It tells the function to return a second table—a detailed report card—that shows exactly why an estimate passed or failed.

### Example C: Generating a Table with Reliability Checks

Let's run our analysis again with the reliability checks turned on.

```{r}
# Generate the table and the reliability metrics
results_list <- svytable1(
  design = adult_design_with_na, 
  strata_var = "ObeseStatus", 
  table_vars = vars_with_missing,
  reliability_checks = TRUE,
  return_metrics = TRUE
)

# Display the formatted table with suppressed estimates
knitr::kable(
  results_list$formatted_table, 
  caption = "Table 3: Table with NCHS Reliability Checks Applied"
)
```

You'll notice asterisks (*) in the table. These are estimates that the function has flagged as unreliable based on the NCHS rules [@nhanes_reliability_estimates]. To see why, we can look at the metrics table.

```{r}
# Display the detailed reliability metrics
knitr::kable(
  results_list$reliability_metrics, 
  caption = "Detailed Reliability Metrics for Each Estimate"
)
```

## Interpreting the Reliability Metrics

The second table, `reliability_metrics`, is the "report card" for each estimate. The `suppressed` column gives the final verdict (`TRUE` or `FALSE`), and the `fail_...` columns tell you exactly which rule was broken.

Here’s what each check means in simple terms:

### `fail_n_30` (Sample Size Rule)

**What it is:** Checks if the actual number of people surveyed for a specific category (n) is at least 30.  
**Why it matters:** Estimates based on fewer than 30 individuals are too unstable to be reliable.

### `fail_eff_n_30` (Effective Sample Size Rule)

**What it is:** Adjusts for survey weights. The "effective" sample size is the equivalent simple random sample size with the same precision.  
**Rule:** If this effective sample size is less than 30, the estimate is flagged.

### `fail_df_8` (Degrees of Freedom Rule)

**What it is:** Measures how complex your survey design is.  
**Why it matters:** If the degrees of freedom are less than 8, the uncertainty estimate is unreliable.

### `fail_ciw_30` & `fail_rciw_130` (Confidence Interval Width Rules)

**What it is:** Evaluates the width of the confidence interval (CI) — the range of uncertainty around your estimate.  
**Why it matters:** Wide CIs indicate poor precision. These rules flag estimates with CIs too wide to be meaningful.

## 4. Checking Estimate Reliability for Means (NCHS Standards)

### fail_rse_30 (Relative Standard Error)

**What it is:**  
The *Relative Standard Error (RSE)* is a measure of an estimate's imprecision relative to its size, calculated as:

\[
RSE = \frac{Standard\ Error}{Estimate} \times 100
\]

An RSE of **30%** has historically been a common cutoff for determining if an estimate is unreliable.

---

**What it's useful for:**

- **Checking Reliability of Means:**  
  RSE is the *current recommended reliability check for means* [@nhanes_reliability_estimates].  
  According to the NCHS guideline, an estimated mean with an **RSE ≥ 30%** should be identified as unreliable.  
  The `svytable1` function automates this check for all numeric variables.

- **Historical Comparison:**  
  RSE allows comparison with older studies or agency standards that still apply the “RSE < 30%” rule for proportions.

- **General Diagnostic:**  
  It provides a quick, familiar *rule of thumb* to gauge the stability of an estimate, complementing the more modern NCHS checks for proportions.

## Extending Reliability Checks to Regression Models

While `svytable1()` focuses on descriptive statistics, a common next step in analysis is fitting a regression model. Assessing the reliability of regression coefficients is just as important as checking descriptive estimates. To support this workflow, the `svyTable1` package now includes `svydiag()`, a helper function for diagnosing the stability of coefficients from `svyglm()` models.

The function provides key metrics recommended for this purpose, such as the **p-value**, **standard error**, and **confidence interval width**. It also includes the Relative Standard Error (RSE) for comparison, though it is not the recommended primary check for regression coefficients due to its tendency to be misleading for estimates near zero.

### Example: Running Diagnostics on a Survey-Weighted Model

Let's fit a logistic regression model to predict obesity (`ObeseStatus`) using the complete-case NHANES data we prepared earlier. We'll then use `svydiag()` to assess the reliability of our model's coefficients.

```{r reg-diagnostics}
# 1. Fit a survey-weighted logistic regression model
# We use the 'adult_design_complete' object from Example B
fit_obesity <- svyglm(
  ObeseStatus ~ Age + Gender + Race1,
  design = adult_design_complete,
  family = quasibinomial()
)

# 2. Run the diagnostics function on the fitted model
diagnostics_table <- svydiag(fit_obesity)

# 3. Display the diagnostics table
knitr::kable(
  diagnostics_table,
  caption = "Reliability Diagnostics for NHANES Obesity Model",
  digits = 3
)
```

## Interpreting the Regression Diagnostics Table

The output from `svydiag()` provides a clear, term-by-term *report card* for your regression model. It helps you evaluate the reliability and interpretability of each coefficient.

### Key Columns

- **Estimate:**  
  The coefficient on the log-odds scale. It represents the change in the log-odds of the outcome per unit change in the predictor.

- **p.value** and **is_significant:**  
  These indicate whether a predictor has a statistically significant association with the outcome.  
  In our example, *Age* and *Race1 (Black)* are significant, while *Gender* is not.

- **CI_Width:**  
  This measures the precision of the estimate — smaller confidence interval widths imply more precise estimates.  
  For instance, the width for *GenderMale* is relatively narrow, but since its estimate is close to zero, it is not statistically significant.

- **RSE_percent** and **is_rse_high:**  
  These columns illustrate the limitation of using **Relative Standard Error (RSE)** in regression contexts.  
  The *GenderMale* coefficient shows a very high RSE (over 200%) because its estimate is near zero, even though its standard error is moderate.  
  This highlights why focusing on **p-values** and **confidence intervals** provides a more reliable assessment of coefficient stability than RSE alone.

---

Overall, `svydiag()` complements traditional regression output by making it easier to identify which predictors are both statistically meaningful and statistically stable under complex survey design.


## Goodness-of-Fit Test for Survey Models

The package also includes `svygof()` to perform the Archer-Lemeshow goodness-of-fit test. This test helps assess how well your logistic regression model fits the data. A non-significant p-value (e.g., > 0.05) suggests that the model is a good fit.

```{r gof-test}
# We use the same model and design from the regression diagnostics example
# 1. Run the goodness-of-fit test
gof_results <- svygof(fit_obesity, adult_design_complete)

# 2. Display the results
knitr::kable(
  gof_results,
  caption = "Archer-Lemeshow Goodness-of-Fit Test for Obesity Model"
)
```

This significant p-value suggests that there is evidence of a poor fit. The model does not accurately predict the outcomes across the different risk groups, indicating that it may be mis-specified or missing important variables or interactions.

## Design-Correct AUC for Model Performance

To evaluate the predictive performance of a model, you can calculate the Area Under the Curve (AUC) using `svyAUC()`. This function correctly accounts for the complex survey design (strata, clusters, and weights) by using a replicate-weights design object, which provides a more accurate estimate of the AUC's variance and confidence interval.

```{r auc-calculation}
# 1. The svyAUC() function requires a replicate-weights design object.
# We convert our existing design object for this purpose.
rep_design <- as.svrepdesign(adult_design_complete)

# 2. Refit the model using the (replicate) design object
fit_obesity_rep <- svyglm(
  ObeseStatus ~ Age + Gender + Race1,
  design = rep_design,
  family = quasibinomial()
)

# 3. Calculate the design-correct AUC
auc_results_list <- svyAUC(fit_obesity_rep, rep_design, plot = TRUE)

# 2. Display the summary table from the list
knitr::kable(
  auc_results_list$summary,
  caption = "Design-Correct AUC for Obesity Model"
)

# Use the roc_data component to build a custom ggplot
library(ggplot2)
ggplot(auc_results_list$roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue", size = 1) +
  geom_abline(linetype = "dashed") +
  labs(
    title = "Survey-Weighted ROC Curve",
    subtitle = paste0("AUC = ", round(auc_results_list$summary$AUC, 3)),
    x = "1 - Specificity (FPR)",
    y = "Sensitivity (TPR)"
  ) +
  theme_minimal()
```

An AUC of 0.5 represents a model with no better-than-random chance of discriminating between outcomes. The model's AUC of 0.587 is very close to this baseline, which indicates poor to failed discrimination. It is not effective at distinguishing between individuals who are obese and those who are not.

## References
